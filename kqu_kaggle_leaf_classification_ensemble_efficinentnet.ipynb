{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "kqu_kaggle_leaf_classification_ensemble_efficinentnet.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "41371d1e9708498bb327a4a02cdf9614": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_e87725ad74764535ba7503125711c35f",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_d43ff98b18d44a5393dbc2d919239d83",
              "IPY_MODEL_b53a9fd4fb2a4f9d92c5a9d2e1375a7f",
              "IPY_MODEL_5253eea01de841eea81a18ed66375944"
            ]
          }
        },
        "e87725ad74764535ba7503125711c35f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "d43ff98b18d44a5393dbc2d919239d83": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_82c5c362f47344a19c4c408426367d15",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 26%",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_22a14b2b82bb4130af1db395a45ba5f5"
          }
        },
        "b53a9fd4fb2a4f9d92c5a9d2e1375a7f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_d293f5e3f2ff4a9a92431468a827bb00",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "",
            "max": 2140,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 560,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_cfc758f3faf8406ea6709302d9062858"
          }
        },
        "5253eea01de841eea81a18ed66375944": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_e310aaf00ed7457ebdb227a303c92745",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 560/2140 [03:25&lt;09:40,  2.72it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_c972ef3a46d5440d8dac685539bdb738"
          }
        },
        "82c5c362f47344a19c4c408426367d15": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "22a14b2b82bb4130af1db395a45ba5f5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "d293f5e3f2ff4a9a92431468a827bb00": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "cfc758f3faf8406ea6709302d9062858": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "e310aaf00ed7457ebdb227a303c92745": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "c972ef3a46d5440d8dac685539bdb738": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v37OdO684nd8",
        "outputId": "17d7d421-a5e9-46fa-cb14-c5ec6671e912"
      },
      "source": [
        "import torch \n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from google.colab import drive\n",
        "from zipfile import ZipFile \n",
        "from sklearn.model_selection import train_test_split\n",
        "import cv2\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "\n",
        "import os\n",
        "import sys\n",
        "from datetime import datetime\n",
        "import random\n",
        "\n",
        "package_paths = ['/content/gdrive/MyDrive/kaggle-models/efficientnet_pytorch-0.7.0', \\\n",
        "                 '/content/gdrive/MyDrive/kaggle-models/FMix-master']\n",
        "\n",
        "for path in package_paths: \n",
        "  sys.path.append(path) # Q: What does this line do?\n",
        "\n",
        "print(sys.path)\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "print(device)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['', '/env/python', '/usr/lib/python36.zip', '/usr/lib/python3.6', '/usr/lib/python3.6/lib-dynload', '/usr/local/lib/python3.6/dist-packages', '/usr/lib/python3/dist-packages', '/usr/local/lib/python3.6/dist-packages/IPython/extensions', '/root/.ipython', '/content/gdrive/MyDrive/kaggle-models/efficientnet_pytorch-0.7.0', '/content/gdrive/MyDrive/kaggle-models/FMix-master']\n",
            "cuda\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XGpb7j13RkeR",
        "outputId": "0a41fa26-4173-44a9-8395-27ba884f45e6"
      },
      "source": [
        "# clear cuda cache\n",
        "torch.cuda.empty_cache()\n",
        "!nvidia-smi"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Wed Feb  3 21:02:38 2021       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 460.39       Driver Version: 418.67       CUDA Version: 10.1     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla V100-SXM2...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   39C    P0    48W / 300W |     10MiB / 16130MiB |      0%      Default |\n",
            "|                               |                      |                 ERR! |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2Co21ZtCXzND",
        "outputId": "0689a2b4-91e1-4255-9479-839f862dc0f0"
      },
      "source": [
        "# manually kill process that takes gpu memory \n",
        "!kill 587\n",
        "!ps -elf | grep python"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/bin/bash: line 0: kill: (587) - No such process\n",
            "4 S root          61       1  0  80   0 - 49629 epoll_ 20:16 ?        00:00:22 /usr/bin/python2 /usr/local/bin/jupyter-notebook --ip=\"172.28.0.2\" --port=9000 --FileContentsManager.root_dir=\"/\" --LargeFileManager.delete_to_trash=False --MappingKernelManager.root_dir=\"/content\"\n",
            "4 S root         288       1  0  80   0 -  4596 wait   20:18 ?        00:00:00 bash -c tail -n +0 -F \"/root/.config/Google/DriveFS/Logs/drive_fs.txt\" | python3 /opt/google/drive/drive-filter.py > \"/root/.config/Google/DriveFS/Logs/timeouts.txt\" \n",
            "4 S root         290     288  0  80   0 -  7304 pipe_r 20:18 ?        00:00:00 python3 /opt/google/drive/drive-filter.py\n",
            "4 S root         699      61 25  80   0 - 1804721 select 21:02 ?      00:00:02 /usr/bin/python3 -m ipykernel_launcher -f /root/.local/share/jupyter/runtime/kernel-065a91d4-0f95-4082-a7a7-ff50cbffce39.json\n",
            "0 S root         724     699  0  80   0 -  9800 wait   21:02 ?        00:00:00 /bin/bash -c ps -elf | grep python\n",
            "0 S root         726     724  0  80   0 -  9644 pipe_r 21:02 ?        00:00:00 grep python\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RmNlAg82Rwtw",
        "outputId": "c062a775-3a5a-408b-8e1b-5ed70dc81aef"
      },
      "source": [
        "!pip install tqdm --upgrade\n",
        "!pip install -U albumentations\n",
        "!pip install timm\n",
        "import timm # Q: What does it do?\n",
        "from tqdm.notebook import tqdm"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already up-to-date: tqdm in /usr/local/lib/python3.6/dist-packages (4.56.0)\n",
            "Requirement already up-to-date: albumentations in /usr/local/lib/python3.6/dist-packages (0.5.2)\n",
            "Requirement already satisfied, skipping upgrade: numpy>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from albumentations) (1.19.5)\n",
            "Requirement already satisfied, skipping upgrade: opencv-python-headless>=4.1.1 in /usr/local/lib/python3.6/dist-packages (from albumentations) (4.5.1.48)\n",
            "Requirement already satisfied, skipping upgrade: scikit-image>=0.16.1 in /usr/local/lib/python3.6/dist-packages (from albumentations) (0.16.2)\n",
            "Requirement already satisfied, skipping upgrade: PyYAML in /usr/local/lib/python3.6/dist-packages (from albumentations) (3.13)\n",
            "Requirement already satisfied, skipping upgrade: scipy in /usr/local/lib/python3.6/dist-packages (from albumentations) (1.4.1)\n",
            "Requirement already satisfied, skipping upgrade: imgaug>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from albumentations) (0.4.0)\n",
            "Requirement already satisfied, skipping upgrade: pillow>=4.3.0 in /usr/local/lib/python3.6/dist-packages (from scikit-image>=0.16.1->albumentations) (7.0.0)\n",
            "Requirement already satisfied, skipping upgrade: PyWavelets>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from scikit-image>=0.16.1->albumentations) (1.1.1)\n",
            "Requirement already satisfied, skipping upgrade: networkx>=2.0 in /usr/local/lib/python3.6/dist-packages (from scikit-image>=0.16.1->albumentations) (2.5)\n",
            "Requirement already satisfied, skipping upgrade: matplotlib!=3.0.0,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from scikit-image>=0.16.1->albumentations) (3.2.2)\n",
            "Requirement already satisfied, skipping upgrade: imageio>=2.3.0 in /usr/local/lib/python3.6/dist-packages (from scikit-image>=0.16.1->albumentations) (2.4.1)\n",
            "Requirement already satisfied, skipping upgrade: six in /usr/local/lib/python3.6/dist-packages (from imgaug>=0.4.0->albumentations) (1.15.0)\n",
            "Requirement already satisfied, skipping upgrade: Shapely in /usr/local/lib/python3.6/dist-packages (from imgaug>=0.4.0->albumentations) (1.7.1)\n",
            "Requirement already satisfied, skipping upgrade: opencv-python in /usr/local/lib/python3.6/dist-packages (from imgaug>=0.4.0->albumentations) (4.1.2.30)\n",
            "Requirement already satisfied, skipping upgrade: decorator>=4.3.0 in /usr/local/lib/python3.6/dist-packages (from networkx>=2.0->scikit-image>=0.16.1->albumentations) (4.4.2)\n",
            "Requirement already satisfied, skipping upgrade: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image>=0.16.1->albumentations) (1.3.1)\n",
            "Requirement already satisfied, skipping upgrade: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image>=0.16.1->albumentations) (0.10.0)\n",
            "Requirement already satisfied, skipping upgrade: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image>=0.16.1->albumentations) (2.4.7)\n",
            "Requirement already satisfied, skipping upgrade: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image>=0.16.1->albumentations) (2.8.1)\n",
            "Requirement already satisfied: timm in /usr/local/lib/python3.6/dist-packages (0.3.4)\n",
            "Requirement already satisfied: torch>=1.4 in /usr/local/lib/python3.6/dist-packages (from timm) (1.7.0+cu101)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.6/dist-packages (from timm) (0.8.1+cu101)\n",
            "Requirement already satisfied: dataclasses in /usr/local/lib/python3.6/dist-packages (from torch>=1.4->timm) (0.8)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch>=1.4->timm) (0.16.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.6/dist-packages (from torch>=1.4->timm) (3.7.4.3)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torch>=1.4->timm) (1.19.5)\n",
            "Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.6/dist-packages (from torchvision->timm) (7.0.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M6OMinjOP5bh"
      },
      "source": [
        "CFG = {\n",
        "    'img_size' : 512,\n",
        "    'epoch_num' : 5,\n",
        "    'fold_num' : 5,\n",
        "    'lr' : 1e-3,\n",
        "    'min_lr' : 1e-6,\n",
        "    'momentum' : 0.9,\n",
        "    'weight_decay':1e-6, # Q: How does weight decay work for Adam?\n",
        "    'train_batch_size' : 8,\n",
        "    'val_batch_size' : 16,\n",
        "    'models' : ['tf_efficientnet_b4_ns'],\n",
        "\n",
        "    'img_path_prefix' : '/content/train_images/',\n",
        "    'train_path_prefix' : '/content/gdrive/MyDrive/kaggle-competition-datasets/cassava-leaf-disease-classification/',\n",
        "    'train_dir_path' : '/content/train_images/',\n",
        "    'model_save_path_prefix' : '/content/gdrive/MyDrive/kaggle-models/kaggle-leaf-classification-models/',\n",
        "    'log_path_prefix' : '/content/gdrive/MyDrive/kaggle-models/kaggle-leaf-classification-models/',\n",
        "    \n",
        "    'range_low' : 0,\n",
        "    'range_high' : 1e6,\n",
        "} # CFG stands for configuration"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Im448aAw-0R6",
        "outputId": "cfde4234-1024-4556-c074-8275d749c88b"
      },
      "source": [
        "# connect with google drive and unzip the zipped image folder\n",
        "drive.mount('/content/gdrive')\n",
        "\n",
        "train_path_prefix = CFG['train_path_prefix']\n",
        "train_zip_path = train_path_prefix + \"train_images.zip\"\n",
        "train_dir_path = CFG['train_dir_path']\n",
        "\n",
        "if not os.path.isdir(train_dir_path):\n",
        "  with ZipFile(train_zip_path, 'r') as zip_f: \n",
        "    zip_f.extractall(path='/content') "
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CuPN3bn7NI8w"
      },
      "source": [
        "# from fmix import sample_mask, make_low_freq_image, binarise_mask"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B4nFSGKs-8ZN"
      },
      "source": [
        "# load leaf images dataset\n",
        "train_images_id = pd.read_csv(train_path_prefix + 'train.csv')['image_id'].to_numpy()\n",
        "train_labels = pd.read_csv(train_path_prefix + 'train.csv')['label'].to_numpy()"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ej2upxd6KFXG",
        "outputId": "97f669c3-c317-44ab-f5ca-7d86ddb1fab7"
      },
      "source": [
        "# define the image-read fucntion\n",
        "def read_img_from_path(path):\n",
        "    im_bgr = cv2.imread(path)\n",
        "    im_rgb = im_bgr[:, :, ::-1].copy() # Q: What does ::-1 do?\n",
        "    return im_rgb\n",
        "\n",
        "test_img = read_img_from_path(CFG['train_dir_path'] + '1000015157.jpg')\n",
        "print(test_img.shape)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(600, 800, 3)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3Av-Xjly49Ci"
      },
      "source": [
        "# define image transformation functions\n",
        "from albumentations.pytorch import ToTensorV2\n",
        "from albumentations import (\n",
        "    HorizontalFlip, VerticalFlip, IAAPerspective, ShiftScaleRotate, CLAHE, RandomRotate90,\n",
        "    Transpose, ShiftScaleRotate, Blur, OpticalDistortion, GridDistortion, HueSaturationValue,\n",
        "    IAAAdditiveGaussianNoise, GaussNoise, MotionBlur, MedianBlur, IAAPiecewiseAffine, RandomResizedCrop,\n",
        "    IAASharpen, IAAEmboss, RandomBrightnessContrast, Flip, OneOf, Compose, Normalize, Cutout, CoarseDropout, ShiftScaleRotate, CenterCrop, Resize\n",
        ") # Q: More information about this library\n",
        "\n",
        "transform = Compose(\n",
        "    [\n",
        "     RandomResizedCrop(CFG['img_size'], CFG['img_size']),\n",
        "     HorizontalFlip(p=0.5), # Q: What does p mean here?\n",
        "     VerticalFlip(p=0.5),\n",
        "     ShiftScaleRotate(p=0.1),\n",
        "     RandomBrightnessContrast(brightness_limit=(-0.1,0.1), contrast_limit=(-0.1, 0.1), p=0.5),\n",
        "     Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225], max_pixel_value=255.0, p=1.0),\n",
        "     ToTensorV2(p=1.0),\n",
        "    ])"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QyTl7h-r5_bG"
      },
      "source": [
        "# split images as validation set and training set by the ratio of 8 : 2\n",
        "# n = len(all_images_id)\n",
        "# train_image_id, val_image_id, train_image_label, \\\n",
        "#   val_image_label = train_test_split(all_images_id, all_labels, test_size = 0.2)"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WbSFlcrzNBKm"
      },
      "source": [
        "# define the training and validaiton dataset\n",
        "class TrainDataset(Dataset):\n",
        "  \"\"\" Leaves Training Dataset \"\"\"\n",
        "  def __init__(self, train_img_id, train_img_label, transform = None):\n",
        "    self.train_img_id = train_img_id\n",
        "    self.train_img_label = train_img_label\n",
        "    self.transform = transform\n",
        "  \n",
        "  def __len__(self):\n",
        "    return len(self.train_img_id)\n",
        "\n",
        "  def __getitem__(self, idx): \n",
        "    img_path = CFG['img_path_prefix'] + str(self.train_img_id[idx])\n",
        "    img = read_img_from_path(img_path)\n",
        "    label = self.train_img_label[idx]\n",
        "    if self.transform:\n",
        "      img = transform(image=img)['image']\n",
        "    return (img, label)\n",
        "\n",
        "class ValDataset(Dataset):\n",
        "  \"\"\" Leaves Validation Dataset \"\"\"\n",
        "  def __init__(self, val_img_id, val_img_label):\n",
        "    self.val_img_id = val_img_id\n",
        "    self.val_img_label = val_img_label\n",
        "    self.transform = transform\n",
        "  \n",
        "  def __len__(self):\n",
        "    return len(self.val_img_id)\n",
        "\n",
        "  def __getitem__(self, idx): \n",
        "    img_path = CFG['img_path_prefix'] + str(self.val_img_id[idx])\n",
        "    img = read_img_from_path(img_path)\n",
        "    if self.transform:\n",
        "      img = transform(image=img)['image']   \n",
        "    label = self.val_img_label[idx]\n",
        "    return (img, label)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K8s87gi66Ozp"
      },
      "source": [
        "# define the training and validation data loader\n",
        "def get_dataloaders(train_idx, val_idx, train_image_ids, train_labels):\n",
        "  train_id, train_label = train_image_ids[train_idx], train_labels[train_idx]\n",
        "  val_id, val_label = train_image_ids[val_idx], train_labels[val_idx]\n",
        "\n",
        "  train_dataset = TrainDataset(train_id, train_label, transform)\n",
        "  val_dataset = ValDataset(val_id, val_label)\n",
        "  \n",
        "  train_dataloader = DataLoader(train_dataset, batch_size=CFG['train_batch_size'], shuffle=True, num_workers=0) # Q: How does num_workers work?\n",
        "  val_dataloader = DataLoader(val_dataset, batch_size=CFG['val_batch_size'], shuffle=True, num_workers=0)\n",
        "  return train_dataloader, val_dataloader"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CQr4zL4q6dRe",
        "outputId": "f2822fcd-1f49-4daa-cf1e-ed20417b7660"
      },
      "source": [
        "# load a pretrained res50 network and modify its last linear layer to \n",
        "# match the number of outputs with this problem \n",
        "num_category = 5\n",
        "resnet50 = torchvision.models.resnet50(pretrained=True)\n",
        "resnet50.fc = nn.Linear(2048, num_category)\n",
        "resnet50.to(device)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ResNet(\n",
              "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
              "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (relu): ReLU(inplace=True)\n",
              "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
              "  (layer1): Sequential(\n",
              "    (0): Bottleneck(\n",
              "      (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): Bottleneck(\n",
              "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (2): Bottleneck(\n",
              "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "  )\n",
              "  (layer2): Sequential(\n",
              "    (0): Bottleneck(\n",
              "      (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): Bottleneck(\n",
              "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (2): Bottleneck(\n",
              "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (3): Bottleneck(\n",
              "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "  )\n",
              "  (layer3): Sequential(\n",
              "    (0): Bottleneck(\n",
              "      (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): Bottleneck(\n",
              "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (2): Bottleneck(\n",
              "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (3): Bottleneck(\n",
              "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (4): Bottleneck(\n",
              "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (5): Bottleneck(\n",
              "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "  )\n",
              "  (layer4): Sequential(\n",
              "    (0): Bottleneck(\n",
              "      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "        (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): Bottleneck(\n",
              "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (2): Bottleneck(\n",
              "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "  )\n",
              "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
              "  (fc): Linear(in_features=2048, out_features=5, bias=True)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OCLVg1hKTi8H"
      },
      "source": [
        "# CustomClassifier\n",
        "class CustomClassifier(nn.Module):\n",
        "    def __init__(self, model_arch, num_class, pretrained=False):\n",
        "        super().__init__()\n",
        "        self.model = timm.create_model(model_arch, pretrained=pretrained)\n",
        "        num_features = self.model.classifier.in_features\n",
        "        self.model.classifier = nn.Linear(num_features, num_class)\n",
        "        '''\n",
        "        self.model.classifier = nn.Sequential(\n",
        "            nn.Dropout(0.3),\n",
        "            nn.Linear(n_features, n_class, bias=True)\n",
        "        )\n",
        "        '''\n",
        "    def forward(self, x):\n",
        "        x = self.model(x)\n",
        "        return x"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FqC06pA_6ohc"
      },
      "source": [
        "# The training function\n",
        "def train(train_images_id, train_labels, num_epochs, model_name, model, criterion, optimizer, save_model=False, pretrained=False):\n",
        "  cur_time = datetime.now().strftime('%Y-%m-%d-%H-%M')\n",
        "  if pretrained:\n",
        "    print('Using pretrained model' + model_name + '...')\n",
        "  # move the training model to the gpu device\n",
        "  if device == 'cuda':\n",
        "    model = model.cuda()\n",
        "  print('Start training...')\n",
        "  for epoch in range(num_epochs): \n",
        "    folds = StratifiedKFold(n_splits=CFG['fold_num'], shuffle=True).split(np.arange(train_images_id.shape[0]), train_labels)\n",
        "    for fold_idx, (train_idx, val_idx) in enumerate(folds):\n",
        "      train_dataloader, val_dataloader = get_dataloaders(train_idx, val_idx, train_images_id, train_labels)\n",
        "      running_loss = 0.0\n",
        "      train_batch_cnt = 0\n",
        "      # training process\n",
        "      # TODO: Try autocast\n",
        "      for data in tqdm(train_dataloader):\n",
        "        inputs, labels = data\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "        train_batch_cnt += 1\n",
        "      print('Training Epoch: [%d] | loss: %.3f' % (epoch + 1, running_loss / train_batch_cnt))\n",
        "\n",
        "      # validaiton process\n",
        "      running_loss = 0.0\n",
        "      correct_prediction_cnt = 0\n",
        "      val_batch_cnt = 0\n",
        "      with torch.no_grad():\n",
        "        for data in tqdm(val_dataloader):\n",
        "          inputs, labels = data\n",
        "          inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "          outputs = model(inputs)\n",
        "          loss = criterion(outputs, labels)\n",
        "\n",
        "          running_loss += loss\n",
        "          correct_prediction_cnt += torch.eq(torch.argmax(outputs, dim=1), labels).sum().item()\n",
        "          val_batch_cnt += 1 \n",
        "        print('Validation Epoch [%d] | loss: %.3f | accuracy: %.3f' % (epoch, running_loss / val_batch_cnt, \\\n",
        "            correct_prediction_cnt * 1.0 / len(val_dataloader.dataset)))\n",
        "        \n",
        "      # Log the current metrics\n",
        "      with open(CFG['log_path_prefix'] + 'log_' + cur_time + '_' + model_name + '.txt', 'a') as f:\n",
        "        f.write('Training Fold %d | Epoch %d\\n' % (fold_idx, epoch))\n",
        "        f.write('Training Loss: %.3f\\n' % (running_loss / train_batch_cnt))\n",
        "        f.write('Validation Loss: %.3f | Accuracy: %.3f \\n\\n' % (running_loss / val_batch_cnt, \\\n",
        "                                                                 correct_prediction_cnt * 1.0 / len(val_dataloader.dataset)))\n",
        "    # Save the trained model for each fold of the data\n",
        "    if save_model:\n",
        "        model_state_save_path = CFG['model_save_path_prefix'] + 'fold_' + str(fold_idx) + '_epoch_' + str(fold_idx * CFG['epoch_num']) + '_' + model_name + '.pt'\n",
        "        torch.save(model.state_dict(), model_state_save_path)"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J7t3LwNe6rx6"
      },
      "source": [
        "# Train resnet50\n",
        "\n",
        "# criterion = nn.CrossEntropyLoss()\n",
        "# resnet50_optimizer = optim.Adam(resnet50.parameters(), lr=CFG['lr'], weight_decay=CFG['weight_decay']) # Q: Characteristics of Adam Optimizer?\n",
        "# train(train_images_id, train_labels, CFG['epoch_num'], \\\n",
        "#       'resnet50' + str(random.randint(CFG['range_low'], CFG['range_high'])), resnet50, criterion, resnet50_optimizer, True)"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 66,
          "referenced_widgets": [
            "41371d1e9708498bb327a4a02cdf9614",
            "e87725ad74764535ba7503125711c35f",
            "d43ff98b18d44a5393dbc2d919239d83",
            "b53a9fd4fb2a4f9d92c5a9d2e1375a7f",
            "5253eea01de841eea81a18ed66375944",
            "82c5c362f47344a19c4c408426367d15",
            "22a14b2b82bb4130af1db395a45ba5f5",
            "d293f5e3f2ff4a9a92431468a827bb00",
            "cfc758f3faf8406ea6709302d9062858",
            "e310aaf00ed7457ebdb227a303c92745",
            "c972ef3a46d5440d8dac685539bdb738"
          ]
        },
        "id": "jTHqA8DfZWQX",
        "outputId": "c3a8722c-5ae0-4def-93e2-660f2adf1e98"
      },
      "source": [
        "# Train efficientnet\n",
        "num_output_category = len(np.unique(train_labels))\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "efficientnet = CustomClassifier(CFG['models'][0], num_output_category, pretrained=True)\n",
        "efficientnet_optimizer = optim.Adam(efficientnet.parameters(), lr=CFG['lr'], weight_decay=CFG['weight_decay'])\n",
        "train(train_images_id, train_labels, CFG['epoch_num'], \\\n",
        "      'efficientnet' + str(random.randint(CFG['range_low'], CFG['range_high'])), efficientnet, criterion, efficientnet_optimizer, True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Start training...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "41371d1e9708498bb327a4a02cdf9614",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "  0%|          | 0/2140 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fLz8Jvs6cBGv"
      },
      "source": [
        "# TODO: load the trained efficientnet model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JQeFavmVVqIl",
        "outputId": "4ea99d01-7b8a-4ce9-bf86-c8095fd2450f"
      },
      "source": [
        "# load the trained resnet50 model\n",
        "pretrained_model_path = '/content/trained_epoch_3_dict_model.pt'\n",
        "resnet50_copy = torchvision.models.resnet50(pretrained=False)\n",
        "resnet50_copy.fc = nn.Linear(2048, num_category)\n",
        "resnet50_copy.load_state_dict(torch.load(pretrained_model_path))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 62
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vSR5ObrqYZXo",
        "outputId": "399b126d-7f69-4532-8ce7-37c87ca2df7e"
      },
      "source": [
        "# load the test images id\n",
        "test_img_path = '/content/test_images'\n",
        "test_img_id = list(os.listdir(test_img_path))\n",
        "print(test_img_id)\n",
        "\n",
        "# create the test dataset\n",
        "class TestDataset(Dataset):\n",
        "  \"\"\" Leaves Test Dataset \"\"\"\n",
        "  def __init__(self, test_img_id, transform = None):\n",
        "    self.test_img_id = test_img_id\n",
        "    self.transform = transform\n",
        "  \n",
        "  def __len__(self):\n",
        "    return len(self.test_img_id)\n",
        "\n",
        "  def __getitem__(self, idx): \n",
        "    img_path_prefix = '/content/test_images/'\n",
        "    img_path = img_path_prefix + str(self.test_img_id[idx])\n",
        "    img = read_img_from_path(img_path)\n",
        "    if self.transform:\n",
        "      img = self.transform(img)\n",
        "    return img, self.test_img_id[idx]\n",
        "\n",
        "# define the test dataloader\n",
        "test_dataset = TestDataset(test_img_id, transform)\n",
        "test_dataloader = DataLoader(test_dataset, batch_size = 1, shuffle = False, num_workers = 0)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['2216849948.jpg']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XSvoPDh6hpOE"
      },
      "source": [
        "# infer the labels of the test dataset\n",
        "res_test_pred_label = []\n",
        "res_test_img_id = []\n",
        "with torch.no_grad():\n",
        "  for tst_img, tst_img_filename in test_dataloader:\n",
        "    resnet50_pred = resnet50_copy(tst_img)\n",
        "    # TODO: Add predictions from efficientnet and avoid the hard-coded ratio\n",
        "    \n",
        "    pred_label = torch.argmax(pred, dim=1).numpy()[0]\n",
        "    tst_img_id = tst_img_filename[0][:-4]\n",
        "    res_test_img_id.append(tst_img_id)\n",
        "    res_test_pred_label.append(pred_label)\n",
        "    \n",
        "# convert the submission to csv\n",
        "output_path = '/content/submission.csv'\n",
        "column_header = ['image_id', 'label']\n",
        "submission = pd.DataFrame(zip(res_test_img_id, res_test_pred_label), columns=column_header)\n",
        "submission.to_csv(path_or_buf = output_path, index = False)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}